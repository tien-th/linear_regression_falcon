{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1) Load the Salary dataset\n",
    "#    (Same link as in your snippet)\n",
    "salary = pd.read_csv('https://github.com/ybifoundation/Dataset/raw/main/Salary%20Data.csv')\n",
    "\n",
    "# Extract features (X) and target (y)\n",
    "X = salary['Experience Years'].values  # shape: (30,) typically\n",
    "y = salary['Salary'].values           # shape: (30,)\n",
    "\n",
    "# Reshape X to be a 2D array for matrix operations: (n_samples, 1)\n",
    "X = X.reshape(-1, 1)\n",
    "\n",
    "# (Optional) Let's normalize data for easier gradient updates\n",
    "# But you can skip this if you want to keep the original scale.\n",
    "X_mean, X_std = X.mean(), X.std()\n",
    "y_mean, y_std = y.mean(), y.std()\n",
    "\n",
    "X_norm = (X - X_mean) / X_std\n",
    "y_norm = (y - y_mean) / y_std\n",
    "\n",
    "# 2) Define helper functions\n",
    "def predict(X, w, b):\n",
    "    \"\"\"\n",
    "    Given input data X (shape (n_samples, 1)), weight w (scalar),\n",
    "    and bias b (scalar), return predictions y_pred.\n",
    "    \"\"\"\n",
    "    return w * X + b\n",
    "\n",
    "def compute_gradients(X, y, w, b):\n",
    "    \"\"\"\n",
    "    Compute dL/dw and dL/db for MSE loss:\n",
    "    L = 1/(2n) * sum( (y_pred - y)^2 ), for n samples\n",
    "    \"\"\"\n",
    "    n = len(X)\n",
    "    y_pred = predict(X, w, b)\n",
    "    error = (y_pred - y)  # shape (n_samples,)\n",
    "\n",
    "    # dL/dw = (1/n) * sum(error * X)\n",
    "    dw = (1/n) * np.sum(error * X)\n",
    "\n",
    "    # dL/db = (1/n) * sum(error)\n",
    "    db = (1/n) * np.sum(error)\n",
    "\n",
    "    return dw, db\n",
    "\n",
    "# 3) Stochastic Gradient Descent (SGD) function\n",
    "def sgd_linear_regression(X, y, lr=0.01, epochs=10):\n",
    "    \"\"\"\n",
    "    Train a linear regression model via basic SGD (batch for simplicity),\n",
    "    logging the regression line each epoch.\n",
    "    Returns final (w, b).\n",
    "    \"\"\"\n",
    "    # Initialize parameters (w, b) randomly\n",
    "    w = np.random.randn()\n",
    "    b = np.random.randn()\n",
    "    \n",
    "    # Number of training examples\n",
    "    n = len(X)\n",
    "    \n",
    "    for epoch in range(1, epochs+1):\n",
    "        # 1) Compute gradients\n",
    "        dw, db = compute_gradients(X, y, w, b)\n",
    "        \n",
    "        # 2) Update parameters\n",
    "        w -= lr * dw\n",
    "        b -= lr * db\n",
    "        \n",
    "        # 3) Plot the data + current regression line after each epoch\n",
    "        plt.figure(figsize=(6,4))\n",
    "        plt.scatter(X, y, label='Data')\n",
    "        \n",
    "        # Create line for the current w, b\n",
    "        # We'll sample across the range of X\n",
    "        X_line = np.linspace(X.min(), X.max(), 100)\n",
    "        y_line = w * X_line + b\n",
    "        plt.plot(X_line, y_line, color='red', label='SGD Model')\n",
    "\n",
    "        plt.title(f'SGD - Epoch {epoch}')\n",
    "        plt.xlabel('X (normalized)' if X_std != 1 else 'X')\n",
    "        plt.ylabel('y (normalized)' if y_std != 1 else 'y')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "    return w, b\n",
    "\n",
    "# 4) SGD with Momentum\n",
    "def sgd_momentum_linear_regression(X, y, lr=0.01, gamma=0.9, epochs=10):\n",
    "    \"\"\"\n",
    "    Train a linear regression model via SGD with Momentum.\n",
    "    gamma is the momentum coefficient (0 < gamma < 1).\n",
    "    Returns final (w, b).\n",
    "    \"\"\"\n",
    "    w = np.random.randn()\n",
    "    b = np.random.randn()\n",
    "    \n",
    "    # Initialize velocity terms for w and b\n",
    "    vw, vb = 0.0, 0.0\n",
    "    \n",
    "    for epoch in range(1, epochs+1):\n",
    "        dw, db = compute_gradients(X, y, w, b)\n",
    "        \n",
    "        # Update velocities\n",
    "        vw = gamma * vw + lr * dw\n",
    "        vb = gamma * vb + lr * db\n",
    "        \n",
    "        # Update parameters\n",
    "        w -= vw\n",
    "        b -= vb\n",
    "        \n",
    "        # Plot after each epoch\n",
    "        plt.figure(figsize=(6,4))\n",
    "        plt.scatter(X, y, label='Data')\n",
    "        \n",
    "        X_line = np.linspace(X.min(), X.max(), 100)\n",
    "        y_line = w * X_line + b\n",
    "        plt.plot(X_line, y_line, color='green', label='SGD+Momentum Model')\n",
    "        \n",
    "        plt.title(f'SGD + Momentum - Epoch {epoch}')\n",
    "        plt.xlabel('X (normalized)' if X_std != 1 else 'X')\n",
    "        plt.ylabel('y (normalized)' if y_std != 1 else 'y')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "    return w, b\n",
    "\n",
    "\n",
    "# 5) Let's run both training procedures on the normalized data\n",
    "print(\"==== Training SGD (no momentum) ====\")\n",
    "w_sgd, b_sgd = sgd_linear_regression(X_norm, y_norm, lr=0.1, epochs=10)\n",
    "\n",
    "print(\"Final parameters from basic SGD:\")\n",
    "print(f\"w = {w_sgd:.4f}, b = {b_sgd:.4f}\")\n",
    "\n",
    "print(\"\\n==== Training SGD + Momentum ====\")\n",
    "w_mom, b_mom = sgd_momentum_linear_regression(X_norm, y_norm, lr=0.1, gamma=0.9, epochs=10)\n",
    "\n",
    "print(\"Final parameters from SGD+Momentum:\")\n",
    "print(f\"w = {w_mom:.4f}, b = {b_mom:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
